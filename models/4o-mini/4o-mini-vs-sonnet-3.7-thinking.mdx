---
title: ''
publishedAt: '2025-04-01'
author: 'Everett Butler'
image: 'model-vs/'
summary: ''
keywords: ''
metaTitle: ''
metaDescription: ''
canonicalUrl: ''
category: tools
---

In recent years, large language models (LLMs) have evolved from simple text generators to complex systems capable of undertaking numerous tasks, one of which is identifying software bugs. In this blog post, we will analyze and compare the bug detection abilities of two such models: OpenAI's 4o-mini and Anthropic's Sonnet 3.7 Thinking. While both models attempt to revolutionize software verification, they take different approaches, raising the question of which is more effective in catching hard-to-find bugs in code.

## Results

Testing these models involved assessing their ability to identify bugs across 210 distinct samples spanning various programming languages, including Python, TypeScript, Go, Rust, and Ruby.

### Overall Performance

Out of the 210 bugs, OpenAI's 4o-mini detected 19, while Anthropic's Sonnet 3.7 Thinking identified 21. These figures, though modest, are significant considering the complexity of the bugs in question.

### Language-Specific Performance

- **Python**: Sonnet 3.7 Thinking captured only 2 bugs out of 42, compared to 4o-mini's 4, highlighting the models' struggle with Python, likely due to its widespread complexity and variance in code patterns.

- **TypeScript**: Here, Sonnet 3.7 Thinking excelled, finding 5 bugs compared to 4o-mini's 2. TypeScript's structured typing appears to play well with the reasoning capabilities of Sonnet 3.7 Thinking.

- **Go**: Both models had similar performance, 3.7 Thinking catching 4 bugs out of 42, compared to 4o-mini's 3. The simplicity and robustness of Go's syntax might limit the advantage reasoning models have.

- **Rust**: Again, Sonnet 3.7 Thinking detected an extra bug (5/41), compared to 4o-mini's 4, suggesting that its reasoning step is beneficial in Rust's complex ownership features.

- **Ruby**: Sonnet 3.7 Thinking found 5 bugs, while 4o-mini detected 6/42, the only language in which 4o-mini had a slight edge. This might be due to Ruby's dynamic nature, although the figures underscore a close contest.

## Thoughts

This analysis suggests that reasoning models like Anthropic's Sonnet 3.7 Thinking have an edge in more niche languages or those with complex semantics not heavily reliant on statistical pattern matching. Its additional reasoning step significantly aids bug detection in languages where training data might be sparse, like Rust and Ruby.

Conversely, OpenAI's 4o-mini, while not attuned to reasoning before response generation, still performs decently, highlighting its robust architecture that values speed and efficiency, crucial for real-time applications.

The variance in results across languages further supports a hybrid approach in future LLM development: balancing the reasoning ability without sacrificing speed or simplicity.

### Interesting Bugs

One notable bug that demonstrated Sonnet 3.7 Thinking's superiority was in a Ruby audio processing library. The error was found in the `TimeStretchProcessor` class, where the `normalize_gain` calculation erroneously used a fixed formula rather than scaling based on the `stretch_factor`. 

Sonnet's ability to catch this bug was likely due to its advanced reasoning process, considering multiple steps and potential audio output scenarios, while 4o-mini, reliant on pattern recognition, missed the critical mishandling of audio normalization dynamics entirely.

Given test input:
- Test Number: Unspecified but highlighted in results as a critical Ruby bug
- Sonnet's Reasoning Output: "The bug in this file was in the TimeStretchProcessor class of a Ruby audio processing library, specifically in how it calculates normalize_gain..."

The enhanced planning in Sonnet's model allowed it to deduce that consistent audio output required gain scaling relative to stretching, unlike OpenAI's model, which possibly did not connect these intricacies without pre-defined patterns.

The remarkable improvement in Anthropic Sonnet 3.7 Thinking for smaller, less common languages suggests that as these models grow and integrate further thinking methodologies, their capacities for bug detection will compellingly increase.

Overall, these results illuminate the substantial promise but also the current limitations of LLMs in software verification â€” offering solid grounds for both excitement and continued exploration.